{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import sparse as sp_sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "\tdata = pd.read_csv(filename, sep='\\t')\n",
    "\tdata['tags'] = data['tags'].apply(literal_eval)\n",
    "\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('data/train.tsv')\n",
    "validation = read_data('data/validation.tsv')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_validation, y_validation = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train[:10], y_train[:10]\n",
    "X_val, y_val = X_validation[:5], y_validation[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ['How to draw a stacked dotplot in R?',\n",
    " 'mysql select all records where a datetime field is less than a specified value mysql',\n",
    " 'How to terminate windows phone 8.1 app',\n",
    " 'get current time in a specific country via jquery',\n",
    " 'Configuring Tomcat to Use SSL',\n",
    " 'Awesome nested set plugin - how to add new children to the tree at various levels plugin',\n",
    " 'How to create map from JSON response in Ruby on Rails 3.8?',\n",
    " 'rspec test if method is called Ruby on Rails',\n",
    " 'SpringBoot Catalina LifeCycle Exception',\n",
    " 'How to import data from excel to mysql import database using import php']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "#print(X_train.shape)\n",
    "#print(X_train, '\\n', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "\n",
    "\ttext = text.lower()\n",
    "\ttext = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\ttext = BAD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "\ttext_tokens = word_tokenize(text)\n",
    "\tfinal_text = \"\"\n",
    "\t#text = [w for w in text_tokens if not w in STOP_WORDS]\n",
    "\tfor token in text_tokens:\n",
    "\t\tif token not in STOP_WORDS:\n",
    "\t\t\tfinal_text += token + \" \"\n",
    "\n",
    "\tfinal_text = final_text.strip()\n",
    "\t#print(final_text)\n",
    "\treturn final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "\texamples = [\"SQL Server(} - any equivalent; of !Excel's CHOOSE@ ~function?\", \"How to free c++ memory vector<int> * arr?\"]\n",
    "\tanswers = [\"sql server equivalent excels choose function\", \"free c++ memory vectorint arr\"]\n",
    "\tfor ex, ans in zip(examples, answers):\n",
    "\t\tif text_prepare(ex) != ans:\n",
    "\t\t\treturn \"Wrong answer for the case: '%s'\" % ex\n",
    "\treturn 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [text_prepare(x) for x in X_train]\n",
    "X_val = [text_prepare(x) for x in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draw stacked dotplot r', 'mysql select records datetime field less specified value mysql', 'terminate windows phone 81 app', 'get current time specific country via jquery', 'configuring tomcat use ssl', 'awesome nested set plugin add new children tree various levels plugin', 'create map json response ruby rails 38', 'rspec test method called ruby rails', 'springboot catalina lifecycle exception', 'import data excel mysql import database using import php']\n"
     ]
    }
   ],
   "source": [
    "print(X_train)#, '\\n', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [word_tokenize(i) for i in X_train]\n",
    "train_words_tokens=[]\n",
    "for each in X_train:\n",
    "\ttokens = word_tokenize(each)\n",
    "\tfor token in tokens:\n",
    "\t\ttrain_words_tokens.append(token)\n",
    "        \n",
    "train_tags_tokens=[]\n",
    "for each in y_train:\n",
    "\t#tokens = word_tokenize(each)\n",
    "\tfor token in each:\n",
    "\t\ttrain_tags_tokens.append(token)\n",
    "\n",
    "fdist_ttags = nltk.FreqDist(train_tags_tokens)\n",
    "\n",
    "tags_counts = dict((word, freq) for word, freq in fdist_ttags.items() if not word.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "['draw', 'stacked', 'dotplot', 'r', 'mysql', 'select', 'records', 'datetime', 'field', 'less', 'specified', 'value', 'mysql', 'terminate', 'windows', 'phone', '81', 'app', 'get', 'current', 'time', 'specific', 'country', 'via', 'jquery', 'configuring', 'tomcat', 'use', 'ssl', 'awesome', 'nested', 'set', 'plugin', 'add', 'new', 'children', 'tree', 'various', 'levels', 'plugin', 'create', 'map', 'json', 'response', 'ruby', 'rails', '38', 'rspec', 'test', 'method', 'called', 'ruby', 'rails', 'springboot', 'catalina', 'lifecycle', 'exception', 'import', 'data', 'excel', 'mysql', 'import', 'database', 'using', 'import', 'php']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_words_tokens))\n",
    "print(train_words_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_twords = nltk.FreqDist(train_words_tokens)\n",
    "words_counts = dict((word, freq) for word, freq in fdist_twords.items() if not word.isdigit())\n",
    "most_common_wordss = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)#[:3]\n",
    "most_common_words = [each[0] for each in most_common_wordss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "['mysql', 'import', 'plugin', 'ruby', 'rails', 'draw', 'stacked', 'dotplot', 'r', 'select', 'records', 'datetime', 'field', 'less', 'specified', 'value', 'terminate', 'windows', 'phone', 'app', 'get', 'current', 'time', 'specific', 'country', 'via', 'jquery', 'configuring', 'tomcat', 'use', 'ssl', 'awesome', 'nested', 'set', 'add', 'new', 'children', 'tree', 'various', 'levels', 'create', 'map', 'json', 'response', 'rspec', 'test', 'method', 'called', 'springboot', 'catalina', 'lifecycle', 'exception', 'data', 'excel', 'database', 'using', 'php']\n"
     ]
    }
   ],
   "source": [
    "print(len(most_common_words))\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DICT_SIZE = 5000\n",
    "DICT_SIZE = len(most_common_words)\n",
    "WORDS_TO_INDEX = {}\n",
    "INDEX_TO_WORDS = {}\n",
    "index = 0\n",
    "\n",
    "for words in most_common_words[:DICT_SIZE]:\n",
    "\tWORDS_TO_INDEX[words] = index\n",
    "\tINDEX_TO_WORDS[index] = words\n",
    "\tindex += 1\n",
    "\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "\tresult_vector = np.zeros(dict_size)\n",
    "\t#all_words = words_to_index.keys()\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfor each in tokens:\n",
    "\t\tif each in ALL_WORDS:\n",
    "\t\t\ti = words_to_index[each]\n",
    "\t\t\tresult_vector[i] += 1\n",
    "\n",
    "\treturn result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import sparse as sp_sparse\n",
    "#X_train_mybag = [my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE) for text in X_train]\n",
    "#X_val, y_val = X_validation[:5], y_validation[:5]\n",
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 57)\n",
      "(5, 57)\n"
     ]
    }
   ],
   "source": [
    "#print(len(X_train_mybag[2]))\n",
    "print(X_train_mybag.shape)\n",
    "print(X_val_mybag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mysql', 'import', 'plugin', 'ruby', 'rails', 'draw', 'stacked', 'dotplot', 'r', 'select', 'records', 'datetime', 'field', 'less', 'specified', 'value', 'terminate', 'windows', 'phone', 'app', 'get', 'current', 'time', 'specific', 'country', 'via', 'jquery', 'configuring', 'tomcat', 'use', 'ssl', 'awesome', 'nested', 'set', 'add', 'new', 'children', 'tree', 'various', 'levels', 'create', 'map', 'json', 'response', 'rspec', 'test', 'method', 'called', 'springboot', 'catalina', 'lifecycle', 'exception', 'data', 'excel', 'database', 'using', 'php']\n"
     ]
    }
   ],
   "source": [
    "#tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=5, max_df=0.90, decode_error='ignore', token_pattern='(\\S+)')\n",
    "print(most_common_words)\n",
    "tfidf_vectorizer = TfidfVectorizer(input=most_common_words, max_features=50)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(5, 34)\n"
     ]
    }
   ],
   "source": [
    "#print(type(X_train_tfidf))\n",
    "#print(X_train_tfidf.todense().shape)\n",
    "#vocabs = tfidf_vectorizer.vocabulary_.keys()\n",
    "#print(vocabs)\n",
    "print(X_train_tfidf.todense().shape)\n",
    "print(X_val_tfidf.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()))\n",
    "y_train = mlb.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 14)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = OneVsRestClassifier(LogisticRegression())\n",
    "#model.fit(X_train_mybag, y_train)\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 34 features per sample; expecting 58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-cb9e9d77a4f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Users\\pradiptadey\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m                 \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_predict_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m                 \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\pradiptadey\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36m_predict_binary\u001b[1;34m(estimator, X)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# probabilities of the positive class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\pradiptadey\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 305\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 34 features per sample; expecting 58"
     ]
    }
   ],
   "source": [
    "model.predict(X_val_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
